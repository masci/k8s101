{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Intro to Kubernetes Welcome to this introductory course to Kubernetes! Photo by Joseph Barrientos on Unsplash Audience If you never operated a cluster, not even as a user, this course is for you. If you had previous exposure to the matter through webinars, blog posts, demos, this course might still be not too boring but if you know the difference between a Pod and a container, a Deployment and a Service, or you know what an Ingress does, well you might skip it. Prerequisites Despite being a 101 course, a solid understanding of Docker fundamentals (containers, images, registry, etc.) is required. Setup Before starting, please install Homebrew and follow these instructions in order to setup your laptop. Some system-wide binaries will be installed with brew while we are going to use asdf-vm for some specific programs like kubectl in order to have a better control over installes versions: Install system-wide required binaries by running brew bundle Install the following asdf-vm plugins by running: asdf plugin-add kubectl asdf plugin-add helm Finally, install versioned dependencies with asdf install Git clone https://github.com/masci/k8s101/ and perform the exercises from the root of the repo kubectl create course You can now move to the first batch of exercises to start the course.","title":"Home"},{"location":"#intro-to-kubernetes","text":"Welcome to this introductory course to Kubernetes! Photo by Joseph Barrientos on Unsplash","title":"Intro to Kubernetes"},{"location":"#audience","text":"If you never operated a cluster, not even as a user, this course is for you. If you had previous exposure to the matter through webinars, blog posts, demos, this course might still be not too boring but if you know the difference between a Pod and a container, a Deployment and a Service, or you know what an Ingress does, well you might skip it.","title":"Audience"},{"location":"#prerequisites","text":"Despite being a 101 course, a solid understanding of Docker fundamentals (containers, images, registry, etc.) is required.","title":"Prerequisites"},{"location":"#setup","text":"Before starting, please install Homebrew and follow these instructions in order to setup your laptop. Some system-wide binaries will be installed with brew while we are going to use asdf-vm for some specific programs like kubectl in order to have a better control over installes versions: Install system-wide required binaries by running brew bundle Install the following asdf-vm plugins by running: asdf plugin-add kubectl asdf plugin-add helm Finally, install versioned dependencies with asdf install Git clone https://github.com/masci/k8s101/ and perform the exercises from the root of the repo","title":"Setup"},{"location":"#kubectl-create-course","text":"You can now move to the first batch of exercises to start the course.","title":"kubectl create course"},{"location":"010-first-contact/","text":"First contact Kubernetes is an orchestrator for deploying containers but you can think about it as an Operating System running your programs. While it's good to know how an Operating System works (and you should keep learning Kubernetes internals beyond this course), as long as you know how to install, upgrade and verify your programs are running fine, that'll be enough. You will always see Kubernetes as a single component but under the hood it runs on a Cluster of machines called Nodes . More nodes means more availability. Bigger nodes means more memory and CPU available to your programs. Every Object managed by Kubernetes is represented by a RESTful resource and we'll see how, at the end of the day, kubectl is just a smart HTTP client. For the scope of this course, we won't need a fully functional multi-node Kubernetes cluster. Instead, we'll be using Kind. KIND stands for Kubernetes IN Docker and it's capable of managing local clusters of one node orchestrating a bunch of Docker containers. At this stage, feel free to ignore the internals and look at Kind for what it is, a tool to start/stop Kubernetes clusters. Another tool you must get familiar with is kubectl , the CLI tool you'll be using to do stuff on any Kubernetes cluster, whatever it's local or a production system. One caveat about kubectl , you need to double check the version you have in your system is compatible with the version of Kubernetes that runs in the cluster: client and server can be at most 1 minor version distant from each other. In our case, the versions are pinned so as long as you've followed the setup instructions you should be good. In this unit we'll start a local Cluster and we'll explore its components with kubectl . Exercise n.1: start the Cluster If kind is correctly installed, all you have to do is running this command from the root of the repo: $ kind create cluster --name k8s101 --image kindest/node:v1.18.2 --config kind-config.yaml Creating cluster \"kind\" ... \u2713 Ensuring node image (kindest/node:v1.18.2) \ud83d\uddbc \u2713 Preparing nodes \ud83d\udce6 \u2713 Writing configuration \ud83d\udcdc \u2713 Starting control-plane \ud83d\udd79\ufe0f \u2713 Installing CNI \ud83d\udd0c \u2713 Installing StorageClass \ud83d\udcbe Set kubectl context to \"kind-kind\" You can now use your cluster with: kubectl cluster-info --context kind-kind Have a nice day! \ud83d\udc4b Notice we use a specific image (and not latest) so that we know it'll work with the pinned kubectl version. If you see no errors the default Cluster called kind should be ready to use, and you can confirm it by running: $ kind get clusters kind Exercise n.2: explore the Cluster To get informations about the Cluster, just run kubectl cluster-info To prove that everything is a RESTful resource in Kubernetes, you can increase kubectl 's log verbosity and see how it performs HTTP calls under the hood: kubectl cluster-info -v6 We can query the status of few key components of the Cluster: kubectl get componentstatuses And we can also get the list of all the nodes composing the Cluster: kubectl get nodes The more detailed, more structured version of kubectl get is the command describe , that gives us a comprehensive view of the Objects we query, in this case the nodes of the Cluster: kubectl describe nodes","title":"First contact"},{"location":"010-first-contact/#first-contact","text":"Kubernetes is an orchestrator for deploying containers but you can think about it as an Operating System running your programs. While it's good to know how an Operating System works (and you should keep learning Kubernetes internals beyond this course), as long as you know how to install, upgrade and verify your programs are running fine, that'll be enough. You will always see Kubernetes as a single component but under the hood it runs on a Cluster of machines called Nodes . More nodes means more availability. Bigger nodes means more memory and CPU available to your programs. Every Object managed by Kubernetes is represented by a RESTful resource and we'll see how, at the end of the day, kubectl is just a smart HTTP client. For the scope of this course, we won't need a fully functional multi-node Kubernetes cluster. Instead, we'll be using Kind. KIND stands for Kubernetes IN Docker and it's capable of managing local clusters of one node orchestrating a bunch of Docker containers. At this stage, feel free to ignore the internals and look at Kind for what it is, a tool to start/stop Kubernetes clusters. Another tool you must get familiar with is kubectl , the CLI tool you'll be using to do stuff on any Kubernetes cluster, whatever it's local or a production system. One caveat about kubectl , you need to double check the version you have in your system is compatible with the version of Kubernetes that runs in the cluster: client and server can be at most 1 minor version distant from each other. In our case, the versions are pinned so as long as you've followed the setup instructions you should be good. In this unit we'll start a local Cluster and we'll explore its components with kubectl .","title":"First contact"},{"location":"010-first-contact/#exercise-n1-start-the-cluster","text":"If kind is correctly installed, all you have to do is running this command from the root of the repo: $ kind create cluster --name k8s101 --image kindest/node:v1.18.2 --config kind-config.yaml Creating cluster \"kind\" ... \u2713 Ensuring node image (kindest/node:v1.18.2) \ud83d\uddbc \u2713 Preparing nodes \ud83d\udce6 \u2713 Writing configuration \ud83d\udcdc \u2713 Starting control-plane \ud83d\udd79\ufe0f \u2713 Installing CNI \ud83d\udd0c \u2713 Installing StorageClass \ud83d\udcbe Set kubectl context to \"kind-kind\" You can now use your cluster with: kubectl cluster-info --context kind-kind Have a nice day! \ud83d\udc4b Notice we use a specific image (and not latest) so that we know it'll work with the pinned kubectl version. If you see no errors the default Cluster called kind should be ready to use, and you can confirm it by running: $ kind get clusters kind","title":"Exercise n.1: start the Cluster"},{"location":"010-first-contact/#exercise-n2-explore-the-cluster","text":"To get informations about the Cluster, just run kubectl cluster-info To prove that everything is a RESTful resource in Kubernetes, you can increase kubectl 's log verbosity and see how it performs HTTP calls under the hood: kubectl cluster-info -v6 We can query the status of few key components of the Cluster: kubectl get componentstatuses And we can also get the list of all the nodes composing the Cluster: kubectl get nodes The more detailed, more structured version of kubectl get is the command describe , that gives us a comprehensive view of the Objects we query, in this case the nodes of the Cluster: kubectl describe nodes","title":"Exercise n.2: explore the Cluster"},{"location":"020-pods/","text":"Pods A Pod is the minimum deployable unit on kubernetes and consists of one or more Docker containers. You can\u2019t \u201crun a container\u201d in a Kubernetes cluster but you can schedule and \u201crun a pod\u201d. If we keep using the Operating System metaphor, a pod would be a process and its containers would be threads: each container has its own life (for example it has separated cgroups) but at the same time it's strongly coupled with the other containers in the pod by having shared namespaces, same hostname, same IP address and same open ports. It's important to note that a pod is atomic and it always runs on one node of the Cluster. In other words, all the containers in a pod always lands in the same Node of the Cluster. Exercise n.1: create a Pod First of all, create a pod called nginx-pod running a single container and pulling the nginx image from Dockerhub: $ kubectl run nginx-pod --generator=run-pod/v1 --image=nginx Flag --generator has been deprecated, has no effect and will be removed in the future. pod/nginx-pod created Please ignore the deprecation warning for now, this command is only meaningful in the context of this course and you wouldn't use it in a real world scenario. Now to get the list of running pods run: $ kubectl get pods NAME READY STATUS RESTARTS AGE nginx-pod 1/1 Running 0 104s As we mentioned in the previous unit, we can get a lot more info about an Object using describe , let's do it with our pod: $ kubectl describe pod nginx-pod Name: nginx-pod Namespace: default ... Finally, we can delete the pod: $ kubectl delete pods nginx-pod pod \"nginx-pod\" deleted Exercise n.2: interact with a Pod Let's create the same pod again but this time we expose its TCP port number 80: kubectl run nginx --generator=run-pod/v1 --image=nginx --port 80 The port is only exposed within the Cluster but there's a way to easily open a connection tunnel to our local host at the port 8080 by running: $ kubectl port-forward nginx-pod 8080:80 Forwarding from 127.0.0.1:8080 -> 80 Forwarding from [::1]:8080 -> 80kubectl port-forward nginx-pod 8080:80 Forwarding from 127.0.0.1:8080 -> 80 Forwarding from [::1]:8080 -> 80 The tunnel will stay open until you hit CTRL+C, in the meantime you can open the browser at http://localhost:8080 and see the pod serving requests. Last but not least, we can also get logs for our pod by running: kubectl logs nginx","title":"Pods"},{"location":"020-pods/#pods","text":"A Pod is the minimum deployable unit on kubernetes and consists of one or more Docker containers. You can\u2019t \u201crun a container\u201d in a Kubernetes cluster but you can schedule and \u201crun a pod\u201d. If we keep using the Operating System metaphor, a pod would be a process and its containers would be threads: each container has its own life (for example it has separated cgroups) but at the same time it's strongly coupled with the other containers in the pod by having shared namespaces, same hostname, same IP address and same open ports. It's important to note that a pod is atomic and it always runs on one node of the Cluster. In other words, all the containers in a pod always lands in the same Node of the Cluster.","title":"Pods"},{"location":"020-pods/#exercise-n1-create-a-pod","text":"First of all, create a pod called nginx-pod running a single container and pulling the nginx image from Dockerhub: $ kubectl run nginx-pod --generator=run-pod/v1 --image=nginx Flag --generator has been deprecated, has no effect and will be removed in the future. pod/nginx-pod created Please ignore the deprecation warning for now, this command is only meaningful in the context of this course and you wouldn't use it in a real world scenario. Now to get the list of running pods run: $ kubectl get pods NAME READY STATUS RESTARTS AGE nginx-pod 1/1 Running 0 104s As we mentioned in the previous unit, we can get a lot more info about an Object using describe , let's do it with our pod: $ kubectl describe pod nginx-pod Name: nginx-pod Namespace: default ... Finally, we can delete the pod: $ kubectl delete pods nginx-pod pod \"nginx-pod\" deleted","title":"Exercise n.1: create a Pod"},{"location":"020-pods/#exercise-n2-interact-with-a-pod","text":"Let's create the same pod again but this time we expose its TCP port number 80: kubectl run nginx --generator=run-pod/v1 --image=nginx --port 80 The port is only exposed within the Cluster but there's a way to easily open a connection tunnel to our local host at the port 8080 by running: $ kubectl port-forward nginx-pod 8080:80 Forwarding from 127.0.0.1:8080 -> 80 Forwarding from [::1]:8080 -> 80kubectl port-forward nginx-pod 8080:80 Forwarding from 127.0.0.1:8080 -> 80 Forwarding from [::1]:8080 -> 80 The tunnel will stay open until you hit CTRL+C, in the meantime you can open the browser at http://localhost:8080 and see the pod serving requests. Last but not least, we can also get logs for our pod by running: kubectl logs nginx","title":"Exercise n.2: interact with a Pod"},{"location":"030-declarative/","text":"Going declarative So far we have operated Kubernetes in an imperative fashion, issuing \"orders\" through kubectl but this is not the way Kubernetes was designed to be used. From now on we\u2019ll be using a declarative approach, writing down in YAML syntax how we want the cluster to be after we run kubectl apply . Such Yaml files are called definitions . Exercise n.1: get familiar with kubectl apply Change directory into the folder definitions before running these commands. Apply the definition named two-containers-pod.yaml and see how one of the containers in the pod will create an index.html file that will be then served by the second container in the same pod: $ kubectl apply -f two-containers-pod.yaml pod/two-containers-pod created Then let's see with a browser what the Nginx in that pod is serving: kubectl port-forward two-containers-pod 8080:80 Open the browser at http://localhost:8080. Finally, delete all the pods running in the Cluster: kubectl delete pods --all","title":"Going declarative"},{"location":"030-declarative/#going-declarative","text":"So far we have operated Kubernetes in an imperative fashion, issuing \"orders\" through kubectl but this is not the way Kubernetes was designed to be used. From now on we\u2019ll be using a declarative approach, writing down in YAML syntax how we want the cluster to be after we run kubectl apply . Such Yaml files are called definitions .","title":"Going declarative"},{"location":"030-declarative/#exercise-n1-get-familiar-with-kubectl-apply","text":"Change directory into the folder definitions before running these commands. Apply the definition named two-containers-pod.yaml and see how one of the containers in the pod will create an index.html file that will be then served by the second container in the same pod: $ kubectl apply -f two-containers-pod.yaml pod/two-containers-pod created Then let's see with a browser what the Nginx in that pod is serving: kubectl port-forward two-containers-pod 8080:80 Open the browser at http://localhost:8080. Finally, delete all the pods running in the Cluster: kubectl delete pods --all","title":"Exercise n.1: get familiar with kubectl apply"},{"location":"040-labels/","text":"Labels Labels are simple key/value pairs attached to any Kubernetes object and used at runtime to query and filter such objects. Labels are heavily used by Kubernetes i tself as we\u2019ll see in a moment. Exercise n.1: list labels and use them in queries Change directory into the folder definitions before running these commands. Start two pods with some labels attached (see their definition files for details): kubectl apply -f foo-pod.yaml kubectl apply -f bar-pod.yaml You can see the attached labels for each pod by running: $ kubectl get pods --show-labels NAME READY STATUS RESTARTS AGE LABELS bar 1/1 Running 0 48s env=staging foo 1/1 Running 0 53s env=prod Now let's see how to list only pods having a label env with prod as value. To use a label in order to filter the output of kubectl get pods , we pass the option --selector to kubectl . You can keep the --show-labels flag to double check the query is working as expected: $ kubectl get pods --selector=\"env=prod\" --show-labels NAME READY STATUS RESTARTS AGE LABELS foo 1/1 Running 0 2m19s env=prod In case you want to see any pod with the label env , whatever its value: kubectl get pods --selector=\"env\" --show-labels Queries can be negated with the ! operator: kubectl get pods --selector=\"\\!env\" --show-labels No resources found in default namespace.","title":"Labels"},{"location":"040-labels/#labels","text":"Labels are simple key/value pairs attached to any Kubernetes object and used at runtime to query and filter such objects. Labels are heavily used by Kubernetes i tself as we\u2019ll see in a moment.","title":"Labels"},{"location":"040-labels/#exercise-n1-list-labels-and-use-them-in-queries","text":"Change directory into the folder definitions before running these commands. Start two pods with some labels attached (see their definition files for details): kubectl apply -f foo-pod.yaml kubectl apply -f bar-pod.yaml You can see the attached labels for each pod by running: $ kubectl get pods --show-labels NAME READY STATUS RESTARTS AGE LABELS bar 1/1 Running 0 48s env=staging foo 1/1 Running 0 53s env=prod Now let's see how to list only pods having a label env with prod as value. To use a label in order to filter the output of kubectl get pods , we pass the option --selector to kubectl . You can keep the --show-labels flag to double check the query is working as expected: $ kubectl get pods --selector=\"env=prod\" --show-labels NAME READY STATUS RESTARTS AGE LABELS foo 1/1 Running 0 2m19s env=prod In case you want to see any pod with the label env , whatever its value: kubectl get pods --selector=\"env\" --show-labels Queries can be negated with the ! operator: kubectl get pods --selector=\"\\!env\" --show-labels No resources found in default namespace.","title":"Exercise n.1: list labels and use them in queries"},{"location":"050-annotations/","text":"Annotations Annotation s are very similar to labels, as they are key/value pairs, with a big difference: annotations can\u2019t be used to query or filter Kubernetes objects. Annotations are meant to carry on information used by other softwares, like auditing or monitoring. TIP: when in doubt, use annotations and \u201cpromote\u201d them to labels later, if and when you need to use them in a query. Exercise n.1: can't query Assuming the pod called foo from the previous unit is still running, we can attach to it an annotation named version with a value of 1 by running: $ kubectl annotate pods foo version=1 pod/foo annotated Despite the annotation, the filter won't work this time and foo won't be listed: $ kubectl get pods --selector=\"version=1\" No resources found in default namespace. Clean up all the pods before moving to the next step: $ kubectl delete pods --all pod \"bar\" deleted pod \"foo\" deleted","title":"Annotations"},{"location":"050-annotations/#annotations","text":"Annotation s are very similar to labels, as they are key/value pairs, with a big difference: annotations can\u2019t be used to query or filter Kubernetes objects. Annotations are meant to carry on information used by other softwares, like auditing or monitoring. TIP: when in doubt, use annotations and \u201cpromote\u201d them to labels later, if and when you need to use them in a query.","title":"Annotations"},{"location":"050-annotations/#exercise-n1-cant-query","text":"Assuming the pod called foo from the previous unit is still running, we can attach to it an annotation named version with a value of 1 by running: $ kubectl annotate pods foo version=1 pod/foo annotated Despite the annotation, the filter won't work this time and foo won't be listed: $ kubectl get pods --selector=\"version=1\" No resources found in default namespace. Clean up all the pods before moving to the next step: $ kubectl delete pods --all pod \"bar\" deleted pod \"foo\" deleted","title":"Exercise n.1: can't query"},{"location":"060-deployments/","text":"Deployments So far we scheduled our pods manually but this way we don't leverage any of the cool features of Kubernetes, like scaling up and down automatically or self-healing the system when needed. A Deployment is an object that logically groups pods together according to our specifications. An example of specification is the number of desired pods that must run at any given time no matter what. If a pod dies, we want Kubernetes to bring another one up to replace it; if pods are more than we asked for, we want Kubernetes to kill as many as needed to match our request. Exercise n.1: manage a deployment Change directory into the folder definitions before running these commands. Create a Deployment called nginx-prod : $ kubectl apply -f nginx-prod.yaml deployment.apps/nginx-prod created Now let's ask kubectl to list all the objects present in the cluster to see what happened: $ kubectl get all NAME READY STATUS RESTARTS AGE pod/nginx-prod-d6fd669f4-8tng2 1/1 Running 0 102s pod/nginx-prod-d6fd669f4-9x57t 1/1 Running 0 102s pod/nginx-prod-d6fd669f4-fpxqw 1/1 Running 0 102s NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/nginx-prod 3/3 3 3 102s NAME DESIRED CURRENT READY AGE replicaset.apps/nginx-prod-d6fd669f4 3 3 3 102s As you can see there are now 3 pods, 1 deployment and 1 object we haven't seen yet, a Replicaset . A replicaset has the only responsibility to keep a certain number of pods running at any time; in this case, since our deployment specs defined 3 replicas for our pod, while creating the deployment Kubernetes also attached a replicaset to it. If you run a kubectl describe on the replicaset, you'll notice that it is under the control of our deployment: $ kubectl describe replicaset <the name of the replicaset> Name: nginx-prod-d6fd669f4 Namespace: default <snip> Controlled By: Deployment/nginx-prod <snip> Now let's see the replicaset in action: we're going to kill one pod and see if another one starts to take its place. $ kubectl delete pod <the name of the pod> pod \"the name of the pod\" deleted Respawning a new pod is almost instantaneous, so by the time you run this command again: kubectl get pods you'll see that a new pod with a different name has started, bringing back the pod count to 3.","title":"Deployments"},{"location":"060-deployments/#deployments","text":"So far we scheduled our pods manually but this way we don't leverage any of the cool features of Kubernetes, like scaling up and down automatically or self-healing the system when needed. A Deployment is an object that logically groups pods together according to our specifications. An example of specification is the number of desired pods that must run at any given time no matter what. If a pod dies, we want Kubernetes to bring another one up to replace it; if pods are more than we asked for, we want Kubernetes to kill as many as needed to match our request.","title":"Deployments"},{"location":"060-deployments/#exercise-n1-manage-a-deployment","text":"Change directory into the folder definitions before running these commands. Create a Deployment called nginx-prod : $ kubectl apply -f nginx-prod.yaml deployment.apps/nginx-prod created Now let's ask kubectl to list all the objects present in the cluster to see what happened: $ kubectl get all NAME READY STATUS RESTARTS AGE pod/nginx-prod-d6fd669f4-8tng2 1/1 Running 0 102s pod/nginx-prod-d6fd669f4-9x57t 1/1 Running 0 102s pod/nginx-prod-d6fd669f4-fpxqw 1/1 Running 0 102s NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/nginx-prod 3/3 3 3 102s NAME DESIRED CURRENT READY AGE replicaset.apps/nginx-prod-d6fd669f4 3 3 3 102s As you can see there are now 3 pods, 1 deployment and 1 object we haven't seen yet, a Replicaset . A replicaset has the only responsibility to keep a certain number of pods running at any time; in this case, since our deployment specs defined 3 replicas for our pod, while creating the deployment Kubernetes also attached a replicaset to it. If you run a kubectl describe on the replicaset, you'll notice that it is under the control of our deployment: $ kubectl describe replicaset <the name of the replicaset> Name: nginx-prod-d6fd669f4 Namespace: default <snip> Controlled By: Deployment/nginx-prod <snip> Now let's see the replicaset in action: we're going to kill one pod and see if another one starts to take its place. $ kubectl delete pod <the name of the pod> pod \"the name of the pod\" deleted Respawning a new pod is almost instantaneous, so by the time you run this command again: kubectl get pods you'll see that a new pod with a different name has started, bringing back the pod count to 3.","title":"Exercise n.1: manage a deployment"},{"location":"070-services/","text":"Services A Service is an object providing network access to a deployment, the most common use case being exposing it to the Internet. Expose is also the jargon used in kubectl , where you use a command called expose to create a service. Exercise n.1: expose a deployment Assuming the nginx-prod deployment from the previous unit is still running, we create a new service by exposing it: $ kubectl expose deployment nginx-prod service/nginx-prod exposed As anticipated, under the hood Kubernetes created the corresponding service object: $ kubectl get services NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE nginx-prod ClusterIP 10.104.193.191 <none> 80/TCP 5s Note the newly created service has ClusterIP as its TYPE and <none> as the EXTERNAL-IP value, meaning the corresponding deployment is only exposed internally within the cluster and won't be reachable from the Internet. A service can be created in several different flavors, called types . The default type is ClusterIP and it doesn't provide any routing facility to let the service be accessed from the outside, reason why you see the external IP is <none> . Let's delete this service and try exposing the deployment with a different service type, LoadBalancer : $ kubectl delete service nginx-prod service \"nginx-prod\" deleted $ kubectl expose deployment nginx-prod --type=LoadBalancer service/nginx-prod exposed This time, something seems to happen as you can see from: kubectl get services EXTERNAL-IP is now <pending> and if your cluster is configured to do so, eventually you'll find a real public IP address listed there and you'll be able to access the service from a host outside the cluster. This is because Kubernetes alone doesn't provide any facility to implement an actual Load Balancer, so it's up to you as the cluster admin to set up the last mile (or your cloud provider). For the scope of the course we can stop here, state will remain forever <pending> but it's not important at this stage, we'll get back to this later, when we'll introduce the Ingress object.","title":"Services"},{"location":"070-services/#services","text":"A Service is an object providing network access to a deployment, the most common use case being exposing it to the Internet. Expose is also the jargon used in kubectl , where you use a command called expose to create a service.","title":"Services"},{"location":"070-services/#exercise-n1-expose-a-deployment","text":"Assuming the nginx-prod deployment from the previous unit is still running, we create a new service by exposing it: $ kubectl expose deployment nginx-prod service/nginx-prod exposed As anticipated, under the hood Kubernetes created the corresponding service object: $ kubectl get services NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE nginx-prod ClusterIP 10.104.193.191 <none> 80/TCP 5s Note the newly created service has ClusterIP as its TYPE and <none> as the EXTERNAL-IP value, meaning the corresponding deployment is only exposed internally within the cluster and won't be reachable from the Internet. A service can be created in several different flavors, called types . The default type is ClusterIP and it doesn't provide any routing facility to let the service be accessed from the outside, reason why you see the external IP is <none> . Let's delete this service and try exposing the deployment with a different service type, LoadBalancer : $ kubectl delete service nginx-prod service \"nginx-prod\" deleted $ kubectl expose deployment nginx-prod --type=LoadBalancer service/nginx-prod exposed This time, something seems to happen as you can see from: kubectl get services EXTERNAL-IP is now <pending> and if your cluster is configured to do so, eventually you'll find a real public IP address listed there and you'll be able to access the service from a host outside the cluster. This is because Kubernetes alone doesn't provide any facility to implement an actual Load Balancer, so it's up to you as the cluster admin to set up the last mile (or your cloud provider). For the scope of the course we can stop here, state will remain forever <pending> but it's not important at this stage, we'll get back to this later, when we'll introduce the Ingress object.","title":"Exercise n.1: expose a deployment"},{"location":"080-ingress/","text":"Ingress In Kubernetes, an Ingress is an object that allows access to your Kubernetes services from outside the Kubernetes cluster. You might say this is also true for deployments exposed through a service of type LoadBalancer , but there's a big difference: an ingress is not tied to any particular service. Similarly to \u201cvirtual hosts\u201d on a traditional HTTP server, Kubernetes can use an ingress as an HTTP-based load balancer to \u201cinspect\u201d the requests and route them to the right service depending on user's configuration. An ingress usually consists of an application called Ingress Controller deployed in the cluster itself. It's important to remember that Kubernetes doesn't ship any Ingress Controller out of the box, so you will always need to install this additional component behind an Ingress object. There is plenty of Ingress Controllers available on the market: Nginx, Traefik, Ambassador just to name a few, so you can pick the one that better suits your use case. For the scope of this course, we'll see how to deploy Nginx as an Ingress Controller for our local tiny cluster. Exercise n.1: deploy an ingress controller There is a specific manifest we can use to install Nginx as an Ingress Controller in a Kind cluster, so the installation step is just: $ kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/master/deploy/static/provider/kind/deploy.yaml namespace/ingress-nginx created <snip> Kubernetes is pretty fast to create the objects needed but the ingress will take a while to start. To be sure that the cluster is ready, run this command that will wait until the Ingress Controller is ready: $ kubectl wait --namespace ingress-nginx \\ --for=condition=ready pod \\ --selector=app.kubernetes.io/component=controller \\ --timeout=90s pod/ingress-nginx-controller-6b69c57f44-8xxpw condition met With the ingress up and running, we can move to the next exercise. Exercise n.2 Change directory into the folder definitions before running the commands. Now that we have an Ingress Controller, let's deploy an ingress resource to route requests from outside our cluster to 2 different services. We're going to deploy 2 services exposing 2 pods that will simply echo a string in response of an HTTP request. The ingress will process the request and forward urls ending with /foo to one service and urls ending with /bar to the other one according to our rules specification: $ kubectl apply -f ingress-example.yaml pod/foo-app created service/foo-service created pod/bar-app created service/bar-service created ingress.extensions/example-ingress created To confirm it's working, run curl on localhost: $ curl http://localhost/foo foo $ curl http://localhost/bar bar If no rule is matched, Nginx will respond with its notorious 404 page: $ curl http://localhost/ <html> <head><title>404 Not Found</title></head> <body> <center><h1>404 Not Found</h1></center> <hr><center>nginx/1.19.1</center> </body> </html>","title":"Ingress"},{"location":"080-ingress/#ingress","text":"In Kubernetes, an Ingress is an object that allows access to your Kubernetes services from outside the Kubernetes cluster. You might say this is also true for deployments exposed through a service of type LoadBalancer , but there's a big difference: an ingress is not tied to any particular service. Similarly to \u201cvirtual hosts\u201d on a traditional HTTP server, Kubernetes can use an ingress as an HTTP-based load balancer to \u201cinspect\u201d the requests and route them to the right service depending on user's configuration. An ingress usually consists of an application called Ingress Controller deployed in the cluster itself. It's important to remember that Kubernetes doesn't ship any Ingress Controller out of the box, so you will always need to install this additional component behind an Ingress object. There is plenty of Ingress Controllers available on the market: Nginx, Traefik, Ambassador just to name a few, so you can pick the one that better suits your use case. For the scope of this course, we'll see how to deploy Nginx as an Ingress Controller for our local tiny cluster.","title":"Ingress"},{"location":"080-ingress/#exercise-n1-deploy-an-ingress-controller","text":"There is a specific manifest we can use to install Nginx as an Ingress Controller in a Kind cluster, so the installation step is just: $ kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/master/deploy/static/provider/kind/deploy.yaml namespace/ingress-nginx created <snip> Kubernetes is pretty fast to create the objects needed but the ingress will take a while to start. To be sure that the cluster is ready, run this command that will wait until the Ingress Controller is ready: $ kubectl wait --namespace ingress-nginx \\ --for=condition=ready pod \\ --selector=app.kubernetes.io/component=controller \\ --timeout=90s pod/ingress-nginx-controller-6b69c57f44-8xxpw condition met With the ingress up and running, we can move to the next exercise.","title":"Exercise n.1: deploy an ingress controller"},{"location":"080-ingress/#exercise-n2","text":"Change directory into the folder definitions before running the commands. Now that we have an Ingress Controller, let's deploy an ingress resource to route requests from outside our cluster to 2 different services. We're going to deploy 2 services exposing 2 pods that will simply echo a string in response of an HTTP request. The ingress will process the request and forward urls ending with /foo to one service and urls ending with /bar to the other one according to our rules specification: $ kubectl apply -f ingress-example.yaml pod/foo-app created service/foo-service created pod/bar-app created service/bar-service created ingress.extensions/example-ingress created To confirm it's working, run curl on localhost: $ curl http://localhost/foo foo $ curl http://localhost/bar bar If no rule is matched, Nginx will respond with its notorious 404 page: $ curl http://localhost/ <html> <head><title>404 Not Found</title></head> <body> <center><h1>404 Not Found</h1></center> <hr><center>nginx/1.19.1</center> </body> </html>","title":"Exercise n.2"},{"location":"090-configmaps/","text":"Config Maps Config maps are a feature of Kubernetes we can use to store configuration parameters that pods running on the cluster can access at runtime. A config map can be filled with data coming from different sources: A Yaml dictionary in a .yaml file A file on disk containing one key per line with the format: my.settings.key=myval A directory containing configuration files in the same format as previous bullet From the command line as args for kubectl Once a ConfigMap resource containing our data is created, a pod can use it in two different ways: By mounting a volume (a file will be created in the pod\u2019s volume for each config key) By injecting environment variables in the pod, each value will be stored in an env var named after the key Exercise n.1: config a Pod Change directory into the folder definitions before running the commands. First of all, we need to create the ConfigMap resource: kubectl apply -f myconfig.yaml You can inspect the contents of the config map by running: kubectl describe configmap my-config We can now use it from a pod by mounting a volume (see the definition file): kubectl apply -f nginx-configmap-pod.yaml At this point, we can verify the pod can actually access the configuration values: $ kubectl exec -it nginx-configmap -- bash $$ ls /etc/config password username $$ cat /etc/config/username ; echo root $$ cat /etc/config/password ; echo mypass","title":"Config Maps"},{"location":"090-configmaps/#config-maps","text":"Config maps are a feature of Kubernetes we can use to store configuration parameters that pods running on the cluster can access at runtime. A config map can be filled with data coming from different sources: A Yaml dictionary in a .yaml file A file on disk containing one key per line with the format: my.settings.key=myval A directory containing configuration files in the same format as previous bullet From the command line as args for kubectl Once a ConfigMap resource containing our data is created, a pod can use it in two different ways: By mounting a volume (a file will be created in the pod\u2019s volume for each config key) By injecting environment variables in the pod, each value will be stored in an env var named after the key","title":"Config Maps"},{"location":"090-configmaps/#exercise-n1-config-a-pod","text":"Change directory into the folder definitions before running the commands. First of all, we need to create the ConfigMap resource: kubectl apply -f myconfig.yaml You can inspect the contents of the config map by running: kubectl describe configmap my-config We can now use it from a pod by mounting a volume (see the definition file): kubectl apply -f nginx-configmap-pod.yaml At this point, we can verify the pod can actually access the configuration values: $ kubectl exec -it nginx-configmap -- bash $$ ls /etc/config password username $$ cat /etc/config/username ; echo root $$ cat /etc/config/password ; echo mypass","title":"Exercise n.1: config a Pod"},{"location":"100-secrets/","text":"Secrets Secret resources are very similar to ConfigMap but they were introduced later, with the goal to store sensitive informations and you should consider this feature still a work in progress, at least in vanilla instance of Kubernetes. In fact at this time Kubernetes secrets are stored in plain-text, base64 encoded format. It's important to stress the fact that Encoding is not Encryption : despite the name and the original design, Kubernetes' secrets can't be trusted to store sensitive informations without providing some kind of encryption on top of them. Some cloud providers encrypt Kubernetes secrets at the application layer, see for example GKE . Exercise n.1: create a secret with kubectl Same as config maps, there are several options to create a secret, for brevity we'll pass our secret strings to kubectl : $ kubectl create secret generic my-secret --from-literal=username=superuser --from-literal=password=topsecret secret/my-secret created You can see what's in your secret by running: $ kubectl describe secret my-secret Name: my-secret <snuip> Data ==== username: 9 bytes password: 9 bytes As you can see, differently from a config map we can't see the plain text version of our data because kubectl encoded the literals in base64 on the fly. Exercise n.2: create a secret manually If we want to manually create a secret, we have to handle encoding ourselves: $ echo -n 'admin' | base64 YWRtaW4= Then we can use the encoded value (e.g.) in a yaml definition like this: apiVersion: v1 kind: Secret metadata: name: mysecret type: Opaque data: username: YWRtaW4= And create the object: $ cat <<EOF | kubectl apply -f - apiVersion: v1 kind: Secret metadata: name: mysecret type: Opaque data: username: YWRtaW4= EOF secret/mysecret created","title":"Secrets"},{"location":"100-secrets/#secrets","text":"Secret resources are very similar to ConfigMap but they were introduced later, with the goal to store sensitive informations and you should consider this feature still a work in progress, at least in vanilla instance of Kubernetes. In fact at this time Kubernetes secrets are stored in plain-text, base64 encoded format. It's important to stress the fact that Encoding is not Encryption : despite the name and the original design, Kubernetes' secrets can't be trusted to store sensitive informations without providing some kind of encryption on top of them. Some cloud providers encrypt Kubernetes secrets at the application layer, see for example GKE .","title":"Secrets"},{"location":"100-secrets/#exercise-n1-create-a-secret-with-kubectl","text":"Same as config maps, there are several options to create a secret, for brevity we'll pass our secret strings to kubectl : $ kubectl create secret generic my-secret --from-literal=username=superuser --from-literal=password=topsecret secret/my-secret created You can see what's in your secret by running: $ kubectl describe secret my-secret Name: my-secret <snuip> Data ==== username: 9 bytes password: 9 bytes As you can see, differently from a config map we can't see the plain text version of our data because kubectl encoded the literals in base64 on the fly.","title":"Exercise n.1: create a secret with kubectl"},{"location":"100-secrets/#exercise-n2-create-a-secret-manually","text":"If we want to manually create a secret, we have to handle encoding ourselves: $ echo -n 'admin' | base64 YWRtaW4= Then we can use the encoded value (e.g.) in a yaml definition like this: apiVersion: v1 kind: Secret metadata: name: mysecret type: Opaque data: username: YWRtaW4= And create the object: $ cat <<EOF | kubectl apply -f - apiVersion: v1 kind: Secret metadata: name: mysecret type: Opaque data: username: YWRtaW4= EOF secret/mysecret created","title":"Exercise n.2: create a secret manually"},{"location":"110-helm/","text":"Helm Helm Helm is a command line application that manages Charts . A Chart is a zip archive containing the definitions for an arbitrary number of Kubernetes resources that an applications might require to work properly. Like other software packages, Charts can be installed, updated or uninstalled from a Kubernetes cluster, provided you have the right set of permissions. There are Charts to easily deploy many well known softwares on Kubernetes: dbms like MySQL or Postgres, full-fledged web applications like Wordpress or Joomla, network servers like Traefik or Ambassador, they can all be installed on a Kubernetes cluster using Helm; and if you have a custom application or service you can still leverage Helm by writing your own Charts. The Helm Hub Charts are stored in a (usually public) file storage with a well known layout called Helm repository . Originally, Helm contributors maintained a central repo of \"official\" charts but as it didn't scale, charts are now stored across many different repositories. To overcome this problem, Helm folks created the Chart Hub , a searchable archive listing all the available Charts and the repositories where you can find them. Exercise n.1: install a Chart Let's search for a chart to install a Mysql database server. We want to list the available charts along with the repositories we can use to fetch them: $ helm search hub mysql URL CHART VERSION APP VERSION DESCRIPTION https://hub.helm.sh/charts/appscode/stash-mysql 8.0.14 8.0.14 stash-mysql - MySQL database backup and restore... https://hub.helm.sh/charts/banzaicloud-stable/tidb 0.0.2 A TiDB Helm chart for Kubernetes https://hub.helm.sh/charts/banzaicloud-stable/p... 0.2.4 v0.11.0 A Helm chart for prometheus mysql exporter with... https://hub.helm.sh/charts/banzaicloud-stable/m... 0.1.0 0.2.0 A Helm chart for deploying the Oracle MySQL Ope... https://hub.helm.sh/charts/bitnami/mariadb 7.7.1 10.3.23 Fast, reliable, scalable, and easy to use open-... https://hub.helm.sh/charts/bitnami/phpmyadmin 6.3.2 5.0.2 phpMyAdmin is an mysql administration frontend https://hub.helm.sh/charts/bitnami/mysql 6.14.7 8.0.21 Chart to create a Highly available MySQL cluster <snip> As you can see, every Chart has a CHART VERSION and an APP VERSION : the first one refers to the version of the Chart itself while the latter refers to the application that will be installed. This versioning schema allows to distribute different versions of an application with the same Chart but different chart maintainers might use different conventions, so don't be puzzled if the version numbers are not familiar and browse the chart's docs to get more details. The first column contains the URL of the docs for the chart: for example, if we open https://hub.helm.sh/charts/bitnami/mariadb we see under the Helm CLI install section that the next step would be adding a repository to our local list: $ helm repo add bitnami https://charts.bitnami.com/bitnami \"bitnami\" has been added to your repositories Before proceeding with the installation we need to introduce some Helm terminology to avoid confusion. When we install a Chart we create a Release of that Chart, and every release has a Release Name that mitght or might not be relevant to the application that's being deployed; Helm encourages to use a so-called \"pet-name\" to name releases. Every Release has a set of Values that can be passed by the user to customize the installation. Let's install MariaDB 10.3 , we'll call this Release rainbow-dash and we want to pass some Values , namely a new database, user and password we want to create: $ helm install rainbow-dash --set db.name=myapp,db.user=dbuser,db.password=s3cr3t bitnami/mariadb NAME: rainbow-dash LAST DEPLOYED: Wed Aug 5 17:24:50 2020 NAMESPACE: default STATUS: deployed REVISION: 1 TEST SUITE: None NOTES: Please be patient while the chart is being deployed <snip> Helm adds labels to the objects it manages so we can list which pods are running after installing the chart with: $ kubectl get pods -l release=rainbow-dash NAME READY STATUS RESTARTS AGE rainbow-dash-mariadb-master-0 1/1 Running 0 119s rainbow-dash-mariadb-slave-0 1/1 Running 0 119s To have an idea of which Values a Chart accepts, refer to its documentation in the Helm Hub, for MariaDB: https://hub.helm.sh/charts/bitnami/mariadb Exercise n.2: update a Chart Release Releases can be updated at any time, for example if you need to pass different Values or if you want to deploy a more recent version of an application. You can look at all the releases currently installed in your cluster by running: $ helm ls NAME NAMESPACE REVISION UPDATED STATUS CHART APP VERSION rainbow-dash default 1 2020-08-05 17:24:50.778728 +0200 CEST deployed mariadb-7.7.2 10.3.23 Every Release has a Revision , which is fundamentally a counter that's incremented every time a Release is updated. Let's try to change the password for the default database user: $ helm upgrade rainbow-dash --reuse-values --set db.password=r0tat3d bitnami/mariadb Release \"rainbow-dash\" has been upgraded. Happy Helming! NAME: rainbow-dash <snip> If you run again helm ls , you'll see how the Revision was bumped: $ helm ls NAME NAMESPACE REVISION UPDATED STATUS CHART APP VERSION rainbow-dash default 2 2020-08-05 17:31:01.911759 +0200 CEST deployed mariadb-7.7.2 10.3.23 For every Release deployed on a cluster, you can list the Values currently in use with the following command: $ helm get values rainbow-dash USER-SUPPLIED VALUES: db: name: myapp password: r0tat3d user: dbuser Note how --reuse-values in the helm upgrade command from above tells Helm to merge all the Values currently in use with the ones overridden with --set ; in this case, the resulting Values are the ones from Revision n.1 with the exception of db.password that was explicitly set. Exercise n.3: rollback a Chart If an application installed with Helm is misbehaving, you can roll it back to a previous Revision with the command: $ helm rollback rainbow-dash Rollback was a success! Happy Helming! To see the history of a Helm release, you can do (note the rollback was registered): $ helm history rainbow-dash REVISION UPDATED STATUS CHART APP VERSION DESCRIPTION 1 Wed Aug 5 17:24:50 2020 superseded mariadb-7.7.2 10.3.23 Install complete 2 Wed Aug 5 17:31:01 2020 superseded mariadb-7.7.2 10.3.23 Upgrade complete 3 Wed Aug 5 17:32:33 2020 deployed mariadb-7.7.2 10.3.23 Rollback to 1","title":"Helm"},{"location":"110-helm/#helm","text":"","title":"Helm"},{"location":"110-helm/#helm_1","text":"Helm is a command line application that manages Charts . A Chart is a zip archive containing the definitions for an arbitrary number of Kubernetes resources that an applications might require to work properly. Like other software packages, Charts can be installed, updated or uninstalled from a Kubernetes cluster, provided you have the right set of permissions. There are Charts to easily deploy many well known softwares on Kubernetes: dbms like MySQL or Postgres, full-fledged web applications like Wordpress or Joomla, network servers like Traefik or Ambassador, they can all be installed on a Kubernetes cluster using Helm; and if you have a custom application or service you can still leverage Helm by writing your own Charts.","title":"Helm"},{"location":"110-helm/#the-helm-hub","text":"Charts are stored in a (usually public) file storage with a well known layout called Helm repository . Originally, Helm contributors maintained a central repo of \"official\" charts but as it didn't scale, charts are now stored across many different repositories. To overcome this problem, Helm folks created the Chart Hub , a searchable archive listing all the available Charts and the repositories where you can find them.","title":"The Helm Hub"},{"location":"110-helm/#exercise-n1-install-a-chart","text":"Let's search for a chart to install a Mysql database server. We want to list the available charts along with the repositories we can use to fetch them: $ helm search hub mysql URL CHART VERSION APP VERSION DESCRIPTION https://hub.helm.sh/charts/appscode/stash-mysql 8.0.14 8.0.14 stash-mysql - MySQL database backup and restore... https://hub.helm.sh/charts/banzaicloud-stable/tidb 0.0.2 A TiDB Helm chart for Kubernetes https://hub.helm.sh/charts/banzaicloud-stable/p... 0.2.4 v0.11.0 A Helm chart for prometheus mysql exporter with... https://hub.helm.sh/charts/banzaicloud-stable/m... 0.1.0 0.2.0 A Helm chart for deploying the Oracle MySQL Ope... https://hub.helm.sh/charts/bitnami/mariadb 7.7.1 10.3.23 Fast, reliable, scalable, and easy to use open-... https://hub.helm.sh/charts/bitnami/phpmyadmin 6.3.2 5.0.2 phpMyAdmin is an mysql administration frontend https://hub.helm.sh/charts/bitnami/mysql 6.14.7 8.0.21 Chart to create a Highly available MySQL cluster <snip> As you can see, every Chart has a CHART VERSION and an APP VERSION : the first one refers to the version of the Chart itself while the latter refers to the application that will be installed. This versioning schema allows to distribute different versions of an application with the same Chart but different chart maintainers might use different conventions, so don't be puzzled if the version numbers are not familiar and browse the chart's docs to get more details. The first column contains the URL of the docs for the chart: for example, if we open https://hub.helm.sh/charts/bitnami/mariadb we see under the Helm CLI install section that the next step would be adding a repository to our local list: $ helm repo add bitnami https://charts.bitnami.com/bitnami \"bitnami\" has been added to your repositories Before proceeding with the installation we need to introduce some Helm terminology to avoid confusion. When we install a Chart we create a Release of that Chart, and every release has a Release Name that mitght or might not be relevant to the application that's being deployed; Helm encourages to use a so-called \"pet-name\" to name releases. Every Release has a set of Values that can be passed by the user to customize the installation. Let's install MariaDB 10.3 , we'll call this Release rainbow-dash and we want to pass some Values , namely a new database, user and password we want to create: $ helm install rainbow-dash --set db.name=myapp,db.user=dbuser,db.password=s3cr3t bitnami/mariadb NAME: rainbow-dash LAST DEPLOYED: Wed Aug 5 17:24:50 2020 NAMESPACE: default STATUS: deployed REVISION: 1 TEST SUITE: None NOTES: Please be patient while the chart is being deployed <snip> Helm adds labels to the objects it manages so we can list which pods are running after installing the chart with: $ kubectl get pods -l release=rainbow-dash NAME READY STATUS RESTARTS AGE rainbow-dash-mariadb-master-0 1/1 Running 0 119s rainbow-dash-mariadb-slave-0 1/1 Running 0 119s To have an idea of which Values a Chart accepts, refer to its documentation in the Helm Hub, for MariaDB: https://hub.helm.sh/charts/bitnami/mariadb","title":"Exercise n.1: install a Chart"},{"location":"110-helm/#exercise-n2-update-a-chart-release","text":"Releases can be updated at any time, for example if you need to pass different Values or if you want to deploy a more recent version of an application. You can look at all the releases currently installed in your cluster by running: $ helm ls NAME NAMESPACE REVISION UPDATED STATUS CHART APP VERSION rainbow-dash default 1 2020-08-05 17:24:50.778728 +0200 CEST deployed mariadb-7.7.2 10.3.23 Every Release has a Revision , which is fundamentally a counter that's incremented every time a Release is updated. Let's try to change the password for the default database user: $ helm upgrade rainbow-dash --reuse-values --set db.password=r0tat3d bitnami/mariadb Release \"rainbow-dash\" has been upgraded. Happy Helming! NAME: rainbow-dash <snip> If you run again helm ls , you'll see how the Revision was bumped: $ helm ls NAME NAMESPACE REVISION UPDATED STATUS CHART APP VERSION rainbow-dash default 2 2020-08-05 17:31:01.911759 +0200 CEST deployed mariadb-7.7.2 10.3.23 For every Release deployed on a cluster, you can list the Values currently in use with the following command: $ helm get values rainbow-dash USER-SUPPLIED VALUES: db: name: myapp password: r0tat3d user: dbuser Note how --reuse-values in the helm upgrade command from above tells Helm to merge all the Values currently in use with the ones overridden with --set ; in this case, the resulting Values are the ones from Revision n.1 with the exception of db.password that was explicitly set.","title":"Exercise n.2: update a Chart Release"},{"location":"110-helm/#exercise-n3-rollback-a-chart","text":"If an application installed with Helm is misbehaving, you can roll it back to a previous Revision with the command: $ helm rollback rainbow-dash Rollback was a success! Happy Helming! To see the history of a Helm release, you can do (note the rollback was registered): $ helm history rainbow-dash REVISION UPDATED STATUS CHART APP VERSION DESCRIPTION 1 Wed Aug 5 17:24:50 2020 superseded mariadb-7.7.2 10.3.23 Install complete 2 Wed Aug 5 17:31:01 2020 superseded mariadb-7.7.2 10.3.23 Upgrade complete 3 Wed Aug 5 17:32:33 2020 deployed mariadb-7.7.2 10.3.23 Rollback to 1","title":"Exercise n.3: rollback a Chart"}]}